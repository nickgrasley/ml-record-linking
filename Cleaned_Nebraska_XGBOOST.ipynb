{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Classification Models for Nebraska Record Linking\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jellyfish'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-be7a52c992ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'R:\\JoePriceResearch\\Python\\Anaconda3\\Lib\\site-packages'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjellyfish\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaro_winkler\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjellyfish\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoundex\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjellyfish\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdamerau_levenshtein_distance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jellyfish'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(r'R:\\JoePriceResearch\\Python\\Anaconda3\\Lib\\site-packages')\n",
    "from jellyfish import jaro_winkler as jw\n",
    "from jellyfish import soundex as sdx\n",
    "from jellyfish import damerau_levenshtein_distance as dl\n",
    "import numpy as np\n",
    "from sklearn import ensemble\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import time\n",
    "#from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Census Files\n",
    "The following is the original code used to read in census files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ark1910', 'event_place', 'event_township', 'fs_ims_image_id', 'pr_age',\n",
      "       'pr_bir_place', 'pr_bir_year_est', 'pr_ethnicity_css',\n",
      "       'pr_fthr_bir_place', 'pr_imm_year', 'pr_marital_status',\n",
      "       'pr_mthr_bir_place', 'pr_name_gn', 'pr_name_surn', 'pr_race_or_color',\n",
      "       'pr_relationship_code', 'pr_sex_code', 'source_household_id',\n",
      "       'source_sheet_nbr_ltr'],\n",
      "      dtype='object')\n",
      "Index(['ark1920', 'event_country', 'event_county', 'event_date',\n",
      "       'event_district', 'event_place', 'event_place_level_1',\n",
      "       'event_place_level_1_type', 'event_place_level_2',\n",
      "       'event_place_level_2_orig', 'event_place_level_2_type',\n",
      "       'event_place_level_3', 'event_place_level_3_orig',\n",
      "       'event_place_level_3_type', 'event_place_level_4',\n",
      "       'event_place_level_4_type', 'event_place_orig', 'event_state',\n",
      "       'event_township', 'event_type', 'event_year', 'ext_2_person_id',\n",
      "       'ext_film_nbr', 'ext_pub_nbr', 'ext_repository_name', 'fs_batch_id',\n",
      "       'fs_batch_locality', 'fs_collection_id', 'fs_das_verify',\n",
      "       'fs_digital_film_nbr', 'fs_film_nbr', 'fs_image_apid', 'fs_image_nbr',\n",
      "       'fs_image_type', 'fs_ims_image_id', 'fs_packet_ltr', 'fs_ppq_id',\n",
      "       'fs_record_group', 'fs_record_id', 'fs_record_nbr', 'fs_sort_key',\n",
      "       'fs_ude_batch_nbr', 'fs_unique_id', 'fs_vis_status', 'image_ark',\n",
      "       'pr_age', 'pr_age_orig', 'pr_bir_place', 'pr_bir_year_est',\n",
      "       'pr_ethnicity_css', 'pr_flag_can_read', 'pr_flag_can_write',\n",
      "       'pr_flag_own_or_rent', 'pr_fthr_bir_place', 'pr_imm_year',\n",
      "       'pr_imm_year_orig', 'pr_marital_status', 'pr_marital_status_orig',\n",
      "       'pr_mthr_bir_place', 'pr_name', 'pr_name_gn', 'pr_name_gn_orig',\n",
      "       'pr_name_orig', 'pr_name_pre', 'pr_name_suf', 'pr_name_surn',\n",
      "       'pr_name_surn_orig', 'pr_name_title', 'pr_name_title_orig',\n",
      "       'pr_race_or_color', 'pr_race_or_color_css', 'pr_race_or_color_orig',\n",
      "       'pr_relationship_code', 'pr_relationship_to_head',\n",
      "       'pr_relationship_to_head_css', 'pr_relationship_to_head_orig',\n",
      "       'pr_sex_code', 'pr_sex_code_orig', 'sort_value', 'source_household_id',\n",
      "       'source_line_nbr', 'source_sheet_ltr', 'source_sheet_nbr',\n",
      "       'source_sheet_nbr_ltr', 'pr_relationship_to_head_norm',\n",
      "       'pr_relationship_to_head_lang', 'event_place_level_4_orig',\n",
      "       'fs_person_ark_fw', 'fs_record_ark_fw', 'fs_unique_id_fw'],\n",
      "      dtype='object')\n",
      "['ark1910']\n",
      "['ark1920', 'event_country', 'event_county', 'event_date', 'event_district', 'event_place_level_1', 'event_place_level_1_type', 'event_place_level_2', 'event_place_level_2_orig', 'event_place_level_2_type', 'event_place_level_3', 'event_place_level_3_orig', 'event_place_level_3_type', 'event_place_level_4', 'event_place_level_4_type', 'event_place_orig', 'event_state', 'event_type', 'event_year', 'ext_2_person_id', 'ext_film_nbr', 'ext_pub_nbr', 'ext_repository_name', 'fs_batch_id', 'fs_batch_locality', 'fs_collection_id', 'fs_das_verify', 'fs_digital_film_nbr', 'fs_film_nbr', 'fs_image_apid', 'fs_image_nbr', 'fs_image_type', 'fs_packet_ltr', 'fs_ppq_id', 'fs_record_group', 'fs_record_id', 'fs_record_nbr', 'fs_sort_key', 'fs_ude_batch_nbr', 'fs_unique_id', 'fs_vis_status', 'image_ark', 'pr_age_orig', 'pr_flag_can_read', 'pr_flag_can_write', 'pr_flag_own_or_rent', 'pr_imm_year_orig', 'pr_marital_status_orig', 'pr_name', 'pr_name_gn_orig', 'pr_name_orig', 'pr_name_pre', 'pr_name_suf', 'pr_name_surn_orig', 'pr_name_title', 'pr_name_title_orig', 'pr_race_or_color_css', 'pr_race_or_color_orig', 'pr_relationship_to_head', 'pr_relationship_to_head_css', 'pr_relationship_to_head_orig', 'pr_sex_code_orig', 'sort_value', 'source_line_nbr', 'source_sheet_ltr', 'source_sheet_nbr', 'pr_relationship_to_head_norm', 'pr_relationship_to_head_lang', 'event_place_level_4_orig', 'fs_person_ark_fw', 'fs_record_ark_fw', 'fs_unique_id_fw']\n",
      "['ark1910']\n",
      "['ark1920', 'event_country', 'event_county', 'event_date', 'event_district', 'event_place_level_1', 'event_place_level_1_type', 'event_place_level_2', 'event_place_level_2_orig', 'event_place_level_2_type', 'event_place_level_3', 'event_place_level_3_orig', 'event_place_level_3_type', 'event_place_level_4', 'event_place_level_4_type', 'event_place_orig', 'event_state', 'event_type', 'event_year', 'ext_2_person_id', 'ext_film_nbr', 'ext_pub_nbr', 'ext_repository_name', 'fs_batch_id', 'fs_batch_locality', 'fs_collection_id', 'fs_das_verify', 'fs_digital_film_nbr', 'fs_film_nbr', 'fs_image_apid', 'fs_image_nbr', 'fs_image_type', 'fs_packet_ltr', 'fs_ppq_id', 'fs_record_group', 'fs_record_id', 'fs_record_nbr', 'fs_sort_key', 'fs_ude_batch_nbr', 'fs_unique_id', 'fs_vis_status', 'image_ark', 'pr_age_orig', 'pr_flag_can_read', 'pr_flag_can_write', 'pr_flag_own_or_rent', 'pr_imm_year_orig', 'pr_marital_status_orig', 'pr_name', 'pr_name_gn_orig', 'pr_name_orig', 'pr_name_pre', 'pr_name_suf', 'pr_name_surn_orig', 'pr_name_title', 'pr_name_title_orig', 'pr_race_or_color_css', 'pr_race_or_color_orig', 'pr_relationship_to_head', 'pr_relationship_to_head_css', 'pr_relationship_to_head_orig', 'pr_sex_code_orig', 'sort_value', 'source_line_nbr', 'source_sheet_ltr', 'source_sheet_nbr', 'pr_relationship_to_head_norm', 'pr_relationship_to_head_lang', 'event_place_level_4_orig', 'fs_person_ark_fw', 'fs_record_ark_fw', 'fs_unique_id_fw']\n",
      "['fs_ims_image_id', 'source_sheet_nbr_ltr']\n",
      "['event_country', 'event_county', 'event_date', 'event_district', 'event_place_level_1', 'event_place_level_1_type', 'event_place_level_2', 'event_place_level_2_orig', 'event_place_level_2_type', 'event_place_level_3', 'event_place_level_3_orig', 'event_place_level_3_type', 'event_place_level_4', 'event_place_level_4_type', 'event_place_orig', 'event_state', 'event_type', 'event_year', 'ext_2_person_id', 'ext_film_nbr', 'ext_pub_nbr', 'ext_repository_name', 'fs_batch_id', 'fs_batch_locality', 'fs_collection_id', 'fs_das_verify', 'fs_digital_film_nbr', 'fs_film_nbr', 'fs_image_apid', 'fs_image_nbr', 'fs_image_type', 'fs_ims_image_id', 'fs_packet_ltr', 'fs_ppq_id', 'fs_record_group', 'fs_record_id', 'fs_record_nbr', 'fs_sort_key', 'fs_ude_batch_nbr', 'fs_unique_id', 'fs_vis_status', 'image_ark', 'pr_age_orig', 'pr_flag_can_read', 'pr_flag_can_write', 'pr_flag_own_or_rent', 'pr_imm_year_orig', 'pr_marital_status_orig', 'pr_name', 'pr_name_gn_orig', 'pr_name_orig', 'pr_name_pre', 'pr_name_suf', 'pr_name_surn_orig', 'pr_name_title', 'pr_name_title_orig', 'pr_race_or_color_css', 'pr_race_or_color_orig', 'pr_relationship_to_head', 'pr_relationship_to_head_css', 'pr_relationship_to_head_orig', 'pr_sex_code_orig', 'sort_value', 'source_line_nbr', 'source_sheet_ltr', 'source_sheet_nbr', 'source_sheet_nbr_ltr', 'pr_relationship_to_head_norm', 'pr_relationship_to_head_lang', 'event_place_level_4_orig', 'fs_person_ark_fw', 'fs_record_ark_fw', 'fs_unique_id_fw']\n"
     ]
    }
   ],
   "source": [
    "c1910 = pd.read_stata(r'R:\\JoePriceResearch\\record_linking\\data\\census_1910\\data\\state_files\\Nebraska.dta')\n",
    "print(c1910.columns)\n",
    "c1920 = pd.read_stata(r'R:\\JoePriceResearch\\record_linking\\data\\census_1920\\data\\state_files\\nebraska.dta')\n",
    "print(c1920.columns)\n",
    "cols = [c for c in c1910.columns if c in c1920.columns]\n",
    "print([c for c in c1910.columns if c not in cols])\n",
    "print([c for c in c1920.columns if c not in cols])\n",
    "rncols = [c for c in cols if c not in ['ark1910','ark1920','source_sheet_nbr_ltr','fs_ims_image_id']]\n",
    "print([c for c in c1910.columns if c not in cols])\n",
    "print([c for c in c1920.columns if c not in cols])\n",
    "keep1910 = [r+'1910' for r in rncols]+['ark1910']\n",
    "keep1920 = [r+'1920' for r in rncols]+['ark1920']\n",
    "rename1910 = dict(zip(rncols,[x+'1910' for x in rncols]))\n",
    "rename1920 = dict(zip(rncols,[x+'1920' for x in rncols]))\n",
    "c1910 = c1910.rename(columns=rename1910)\n",
    "c1920 = c1920.rename(columns=rename1920)\n",
    "print([c for c in c1910.columns if c not in keep1910])\n",
    "print([c for c in c1920.columns if c not in keep1920])\n",
    "c1910 = c1910[keep1910]\n",
    "c1920 = c1920[keep1920]\n",
    "c1910 = c1910.rename(columns={'pr_name_surn1910':'last1910'})\n",
    "c1920 = c1920.rename(columns={'pr_name_surn1920':'last1920'})\n",
    "del cols, keep1910, keep1920, rename1910, rename1920, rncols\n",
    "c1910['first_init1910'] = [x[0] if len(x)>0 else '' for x in c1910['pr_name_gn1910']]\n",
    "c1920['first_init1920'] = [x[0] if len(x)>0 else '' for x in c1920['pr_name_gn1920']]\n",
    "c1910['last_init1910'] = [x[0] if len(x)>0 else '' for x in c1910['last1910']]\n",
    "c1920['last_init1920'] = [x[0] if len(x)>0 else '' for x in c1920['last1920']]\n",
    "c1910['first_sdx1910'] = [sdx(x.split()[0]) if len(x)>0 else '' for x in c1910['pr_name_gn1910']]\n",
    "c1920['first_sdx1920'] = [sdx(x.split()[0]) if len(x)>0 else '' for x in c1920['pr_name_gn1920']]\n",
    "c1910['last_sdx1910'] = [sdx(x) for x in c1910['last1910']]\n",
    "c1920['last_sdx1920'] = [sdx(x) for x in c1920['last1920']]\n",
    "c1910['pr_age1910'] = [int(x) if len(x)>0 else np.nan for x in c1910['pr_age1910']]\n",
    "c1920['pr_age1920'] = [int(x) if len(x)>0 else np.nan for x in c1920['pr_age1920']]\n",
    "c1910['pr_imm_year1910'] = [int(x) if len(x)>0 else np.nan for x in c1910['pr_imm_year1910']]\n",
    "c1920['pr_imm_year1920'] = [int(x) if len(x)>0 else np.nan for x in c1920['pr_imm_year1920']]\n",
    "c1920 = c1920[c1920['pr_age1920']>9]\n",
    "keep = [bool(1-(x>1911 and x<1921)) for x in c1920['pr_imm_year1920']]\n",
    "c1920 = c1920[keep]\n",
    "del keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bins, Features, Merge True Matches, Identify False Pairings\n",
    "I used Isaac's origninal method for creating bins and putting the data together. Currently I'm working on binning based on results of our probit model, but I'm still figuring out how to get that to work efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1910['bin'] = [x[0]+x[1]+x[2]+x[3]+x[4]+x[5]+x[6] for x in zip(c1910['first_sdx1910'],c1910['last_sdx1910'],c1910['pr_bir_place1910'],c1910['pr_race_or_color1910'],c1910['pr_sex_code1910'],c1910['pr_mthr_bir_place1910'],c1910['pr_fthr_bir_place1910'])]\n",
    "c1920['bin'] = [x[0]+x[1]+x[2]+x[3]+x[4]+x[5]+x[6] for x in zip(c1920['first_sdx1920'],c1920['last_sdx1920'],c1920['pr_bir_place1920'],c1920['pr_race_or_color1920'],c1920['pr_sex_code1920'],c1920['pr_mthr_bir_place1920'],c1920['pr_fthr_bir_place1920'])]\n",
    "bins = list(c1910['bin'])+list(c1920['bin'])\n",
    "bins = list(set(bins))\n",
    "bindict = dict(zip(bins, range(len(bins))))\n",
    "c1910['bin'] = c1910['bin'].apply(lambda x:bindict[x])\n",
    "c1920['bin'] = c1920['bin'].apply(lambda x:bindict[x])\n",
    "del bins, bindict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(c1910, c1920, on='bin', how='inner')\n",
    "#del c1910_1, c1920_1\n",
    "df['age_diff'] = [abs(x[1]-x[0]-10) for x in zip(df['pr_age1910'],df['pr_age1920'])]\n",
    "df = df[df['age_diff']<4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['county1910'] = [x.split(', ')[-3] if x.count(', ')>1 else '' for x in df['event_place1910']]\n",
    "df['county1920'] = [x.split(', ')[-3] if x.count(', ')>1 else '' for x in df['event_place1920']]\n",
    "df['township1910'] = [re.sub('^ +| +$| ?[0-9]+| ?Ward| ?Precinct', '', t) for t in df['event_township1910']]\n",
    "df = df.drop('event_township1910', axis=1)\n",
    "df['township1920'] = [re.sub('^ +| +$| ?[0-9]+| ?Ward| ?Precinct', '', t) for t in df['event_township1920']]\n",
    "df = df.drop('event_township1920', axis=1)\n",
    "df['first1910'] = [x.split()[0] if len(x)>0 else '' for x in df['pr_name_gn1910']]\n",
    "df['first1920'] = [x.split()[0] if len(x)>0 else '' for x in df['pr_name_gn1920']]\n",
    "df['second1910'] = [x.split()[1] if ' ' in x else '' for x in df['pr_name_gn1910']]\n",
    "df['second1920'] = [x.split()[1] if ' ' in x else '' for x in df['pr_name_gn1920']]\n",
    "df['mid_init1910'] = [x[0] if len(x)>0 else '' for x in df['second1910']]\n",
    "df['mid_init1920'] = [x[0] if len(x)>0 else '' for x in df['second1920']]\n",
    "df['second_sdx1910'] = [sdx(x) for x in df['second1910']]\n",
    "df['second_sdx1920'] = [sdx(x) for x in df['second1920']]\n",
    "\n",
    "df['first_match'] = [x[0]==x[1] for x in zip(df['first1910'],df['first1920'])]\n",
    "df['last_match'] = [x[0]==x[1] for x in zip(df['last1910'],df['last1920'])]\n",
    "df['f1_match'] = df['first_init1910']==df['first_init1920']\n",
    "df['m1_match'] = df['mid_init1910']==df['mid_init1920']\n",
    "df['l1_match'] = df['last_init1910']==df['last_init1920']\n",
    "df['f1_noconflict'] = [x[0]==x[1] or len(x[0])==0 or len(x[1])==0 for x in zip(df['first_init1910'],df['first_init1920'])]\n",
    "df['m1_noconflict'] = [x[0]==x[1] or len(x[0])==0 or len(x[1])==0 for x in zip(df['mid_init1910'],df['mid_init1920'])]\n",
    "df['l1_noconflict'] = [x[0]==x[1] or len(x[0])==0 or len(x[1])==0 for x in zip(df['last_init1910'],df['last_init1920'])]\n",
    "df['first_sdx_match'] = [x[0]==x[1] for x in zip(df['first_sdx1910'],df['first_sdx1920'])]\n",
    "df['last_sdx_match'] = [x[0]==x[1] for x in zip(df['last_sdx1910'],df['last_sdx1920'])]\n",
    "df['first_jw'] = [jw(x[0],x[1]) for x in zip(df['first1910'],df['first1920'])]\n",
    "df['last_jw'] = [jw(x[0],x[1]) for x in zip(df['last1910'],df['last1920'])]\n",
    "df['mid_jw'] = [jw(x[0],x[1]) for x in zip(df['second1910'],df['second1920'])]\n",
    "#df['pr_age1910'] = [int(x) if len(x)>0 else np.nan for x in df['pr_age1910']]\n",
    "#df['pr_age1920'] = [int(x) if len(x)>0 else np.nan for x in df['pr_age1920']]\n",
    "df['age_pm1'] = [abs(x[0]-x[1]) in range(9,12) for x in zip(df['pr_age1910'],df['pr_age1920'])]\n",
    "df['age_pm2'] = [abs(x[0]-x[1]) in range(8,13) for x in zip(df['pr_age1910'],df['pr_age1920'])]\n",
    "df['age_pm3'] = [abs(x[0]-x[1]) in range(7,14) for x in zip(df['pr_age1910'],df['pr_age1920'])]\n",
    "df['mbp_match'] = df['pr_mthr_bir_place1910']==df['pr_mthr_bir_place1920']\n",
    "df['fbp_match'] = df['pr_fthr_bir_place1910']==df['pr_fthr_bir_place1920']\n",
    "df['race_match'] = df['pr_race_or_color1910']==df['pr_race_or_color1920']\n",
    "df['ethn_match'] = df['pr_ethnicity_css1910']==df['pr_ethnicity_css1920']\n",
    "df['sex_match'] = df['pr_sex_code1910']==df['pr_sex_code1920']\n",
    "df['town_match'] = df['township1910']==df['township1920']\n",
    "df['county_match'] = df['county1910']==df['county1920']\n",
    "df['first_xjw12'] = [jw(x[0],x[1]) for x in zip(df['first1910'],df['second1920'])]\n",
    "df['first_xjw21'] = [jw(x[0],x[1]) for x in zip(df['second1910'],df['first1920'])]\n",
    "df['first_xsdx12'] = [x[0]==x[1] for x in zip(df['first_sdx1910'],df['second_sdx1920'])]\n",
    "df['first_xsdx21'] = [x[0]==x[1] for x in zip(df['second_sdx1910'],df['first_sdx1920'])]\n",
    "df['first_maxjw'] = [max(x) for x in zip(df['first_jw'],df['first_xjw12'],df['first_xjw21'])]\n",
    "df['first_maxsdx'] = [max(x) for x in zip(df['first_sdx_match'],df['first_xsdx12'],df['first_xsdx21'])]\n",
    "df['min_jw'] = [min(x) for x in zip(df['first_maxjw'],df['last_jw'])]\n",
    "df['imm_match'] = df['pr_imm_year1910']==df['pr_imm_year1920']\n",
    "\n",
    "features = ['first_match','last_match','f1_match','m1_match','l1_match','f1_noconflict',\n",
    "            'm1_noconflict','l1_noconflict','first_sdx_match','last_sdx_match',\n",
    "            'first_jw','last_jw','mid_jw','age_diff','age_pm1',\n",
    "            'age_pm2','age_pm3','mbp_match','fbp_match','race_match','ethn_match',\n",
    "            'sex_match','town_match','county_match','first_maxjw','first_maxsdx',\n",
    "            'min_jw','imm_match']\n",
    "df['total'] = [np.sum([i==1 for i in x]) for x in list(zip(*[list(df[c]) for c in df.columns]))]\n",
    "features += ['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = pd.read_stata(r'R:\\JoePriceResearch\\record_linking\\data\\crosswalks\\arkhints1910_1920.dta')\n",
    "arks1 = set(df['ark1910'])\n",
    "arks2 = set(df['ark1920'])\n",
    "fs = [x for x in zip(fs['ark1910'],fs['ark1920']) if x[0] in arks1]\n",
    "fs = [x for x in fs if x[1] in arks2]\n",
    "fs = pd.DataFrame(fs)\n",
    "fs['true'] = [1]*fs.shape[0]\n",
    "fs = fs.rename(columns={0:'ark1910',1:'ark1920'})\n",
    "df = pd.merge(df, fs, on=['ark1910','ark1920'], how='left')\n",
    "df['true'] = [x==1 for x in df['true']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marsdena\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:180: UserWarning: evaluating in Python space because the '+' operator is not supported by numexpr for the bool dtype, use '|' instead\n",
      "  .format(op=op_str, alt_op=unsupported[op_str]))\n"
     ]
    }
   ],
   "source": [
    "group = df[['ark1920','true']].groupby(df['ark1920'])\n",
    "train = dict(group['true'].max())\n",
    "df['train2'] = df['ark1920'].apply(lambda x:train[x])\n",
    "group = df[['ark1910','true']].groupby(df['ark1910'])\n",
    "train = dict(group['true'].max())\n",
    "df['train1'] = df['ark1910'].apply(lambda x:train[x])\n",
    "train = df[df['train1']+df['train2']>0].drop(['train1','train2'], axis=1)\n",
    "pred = df[df['train1']+df['train2']==0].drop(['train1','train2'], axis=1) \n",
    "train = train[['true','ark1910','ark1920']+features]   \n",
    "pred = pred[['ark1910','ark1920']+features]   \n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting our Models Together:\n",
    "### First I perform a train-test split on the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 21\n",
    "np.random.seed(seed)\n",
    "train = train.drop(['ark1910','ark1920'],axis=1)\n",
    "y = train['true']\n",
    "X = train.drop(['true'],1)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "The timeit is used to measure the processing time of each model. This model here is gradient boosting. I used the one imported from sklearn's ensemble package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315.7601168869999\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "tic = timeit.default_timer()\n",
    "model = ensemble.GradientBoostingClassifier(n_estimators = 1000, learning_rate = 0.1, max_depth = 6, max_features = 0.8, random_state = 5)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "time = toc - tic\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Measures:\n",
    "First I created a confusion matrix to caclulate precision and recall. I do this again with the classification_report tool from sklearn which makes it easier to get these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16674,  4768],\n",
       "       [ 2293, 65287]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predictions = cross_val_predict(model, X_test, y_test, cv=3)\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16674  4768]\n",
      " [ 2293 65287]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.87      0.80      0.84     21442\n",
      "       True       0.94      0.96      0.95     67580\n",
      "\n",
      "avg / total       0.92      0.92      0.92     89022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is another measure of the model's score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample predictions: 0.9322773993211853\n"
     ]
    }
   ],
   "source": [
    "in_rf_acc = model.score(X_train, y_train)\n",
    "print('In sample predictions: ' + str(in_rf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "Essentially the same as Gradient Boosting but with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86.89002848399991\n"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "model = ensemble.RandomForestClassifier(n_estimators = 1000, max_features = 0.8, max_depth = 6, n_jobs = -1)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "time = toc - tic\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15788,  5654],\n",
       "       [ 1403, 66177]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = cross_val_predict(model, X_test, y_test, cv=3)\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15788  5654]\n",
      " [ 1403 66177]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.83      0.84      0.83     21442\n",
      "       True       0.95      0.95      0.95     67580\n",
      "\n",
      "avg / total       0.92      0.92      0.92     89022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In sample predictions: 0.9215319066990829\n"
     ]
    }
   ],
   "source": [
    "in_rf_acc = model.score(X_train, y_train)\n",
    "print('In sample predictions: ' + str(in_rf_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-35097e8de982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "tic = timeit.default_timer()\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "time = toc - tic\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(model, X_test, y_test, cv=3)\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = timeit.default_timer()\n",
    "model = MLPClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "time = toc - tic\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cross_val_predict(model, X_test, y_test, cv=3)\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, predictions))\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST\n",
    "Here is the model I most recently used for the XGBClassifier. I can't run it on here because it won't let me download the library on any school computer. So I've been saving my training data and running it on my personal. To test it, I used the same commands as before with random forests and gradient boosting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier \n",
    "tic = timeit.default_timer()\n",
    "model = XGBClassifier(max_depth=6, learning_rate = 0.1, n_estimators = 1000)\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "toc = timeit.default_timer()\n",
    "\n",
    "time = toc - tic\n",
    "print(time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
