{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Splycer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I demonstrate the process of running Splycer from start to finish. First, load in all of the necessary modules. In the future, `sys.path.append` will not be necessary since this will be a package built in your Python environment. Make sure you are also in the example folder, which you can change with the `os.chdir()` command listed below.\n",
    "\n",
    "For code clarity, I turned warnings off in this notebook. Don't do this in your future code because you can get valuable warnings sometimes, especially about invalid values during comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "sys.path.append(\"R:/JoePriceResearch/record_linking/projects/deep_learning/ml-record-linking\")\n",
    "from record_set import RecordDataFrame\n",
    "from pairs_set import PairsCOO\n",
    "from feature_engineer import FeatureEngineer\n",
    "from xgboost_match import XGBoostMatch\n",
    "os.chdir(\"R:/JoePriceResearch/record_linking/projects/deep_learning/ml-record-linking/example\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record Set Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell creates record set objects. A record set object is a container for record information that a linker algorithm can access with a unique identifier. In this tutorial, the record set object's internal structure is a [Pandas DataFrame](https://towardsdatascience.com/a-quick-introduction-to-the-pandas-python-library-f1b678f34673). One innovation of our code is that it does not matter what your data's internal structure is as long as you use/create a [wrapper](https://en.wikipedia.org/wiki/Wrapper_function) for it. This gives you the freedom to use the data format that is best for your linking project. I will have some documentation for how to do that, but I have created several common data structures to choose from.\n",
    "\n",
    "(Note: the feather file format I am reading here is a special binary format that Python, R, Julia, etc. can understand. It's extremely fast, so I use it here to avoid long wait times loading the data. However, I wouldn't recommend this format for your projects since other programs can't understand it and it has some limiting design choices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delaware_1910 = RecordDataFrame(2, pd.read_feather(\"delaware_1910.feather\", nthreads=4).set_index(\"index\", drop=True))\n",
    "delaware_1920 = RecordDataFrame(3, pd.read_feather(\"delaware_1920.feather\", nthreads=4).set_index(\"index\", drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what the data look like. You can see there's some missing values. If I was actually running predictions, I would fix these missing values, and I should fix the missing values in these files in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marstat</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>household</th>\n",
       "      <th>immigration</th>\n",
       "      <th>race</th>\n",
       "      <th>rel</th>\n",
       "      <th>female</th>\n",
       "      <th>bp</th>\n",
       "      <th>mbp</th>\n",
       "      <th>fbp</th>\n",
       "      <th>...</th>\n",
       "      <th>last_vec192</th>\n",
       "      <th>last_vec193</th>\n",
       "      <th>last_vec194</th>\n",
       "      <th>last_vec195</th>\n",
       "      <th>last_vec196</th>\n",
       "      <th>last_vec197</th>\n",
       "      <th>last_vec198</th>\n",
       "      <th>last_vec199</th>\n",
       "      <th>last_vec200</th>\n",
       "      <th>last_vec201</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8361303</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1874.0</td>\n",
       "      <td>1934610.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>46</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8437960</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1874.0</td>\n",
       "      <td>1955405.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8387550</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1905.0</td>\n",
       "      <td>1975755.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529805</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>1954502.0</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529824</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1878.0</td>\n",
       "      <td>1954502.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 430 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         marstat  birth_year  household  immigration  race  rel  female  bp  \\\n",
       "index                                                                         \n",
       "8361303      1.0      1874.0  1934610.0          NaN     0    1       1  65   \n",
       "8437960      0.0      1874.0  1955405.0          NaN     0   -1       1  55   \n",
       "8387550      0.0      1905.0  1975755.0          NaN     0    3       1  18   \n",
       "8529805      0.0      1880.0  1954502.0       1880.0     0   -1       1  10   \n",
       "8529824      0.0      1878.0  1954502.0          NaN     0   -1       1  55   \n",
       "\n",
       "         mbp  fbp  ...  last_vec192  last_vec193  last_vec194  last_vec195  \\\n",
       "index              ...                                                       \n",
       "8361303   46   36  ...          NaN          NaN          NaN          NaN   \n",
       "8437960   12   55  ...          NaN          NaN          NaN          NaN   \n",
       "8387550   18   18  ...          NaN          NaN          NaN          NaN   \n",
       "8529805   10   10  ...          NaN          NaN          NaN          NaN   \n",
       "8529824   55   55  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "         last_vec196  last_vec197  last_vec198  last_vec199  last_vec200  \\\n",
       "index                                                                      \n",
       "8361303          NaN          NaN          NaN          NaN          NaN   \n",
       "8437960          NaN          NaN          NaN          NaN          NaN   \n",
       "8387550          NaN          NaN          NaN          NaN          NaN   \n",
       "8529805          NaN          NaN          NaN          NaN          NaN   \n",
       "8529824          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "         last_vec201  \n",
       "index                 \n",
       "8361303          NaN  \n",
       "8437960          NaN  \n",
       "8387550          NaN  \n",
       "8529805          NaN  \n",
       "8529824          NaN  \n",
       "\n",
       "[5 rows x 430 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delaware_1910.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pairs Set Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the pairs set. This object contains pairs of record ids that we want to predict whether they are a match, indexed by the unique identifier in the record set objects. Once again, internal data structure doesn't matter as long as you use/create a wrapper for your specific data structure. Here, I am using a [coordinate sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compares = pd.read_csv(\"delaware_compares.csv\")\n",
    "compares = PairsCOO(2,3, compares.index_1910.values, compares.index_1920.values, np.ones(compares.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Records with Feature Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the comparison engineer is the most intensive part of the pipeline. It is constructed by incrementally adding functions that measure record similarity. For each comparison, you must specify the column name(s) of the record you want to compare; the type of comparison; and any extra arguments that the comparison takes. On theme, you can use any similarity score you want as long as you create a wrapper for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose I wanted to compare first name using jaro-winkler score. After creating a ComparisonEngineer object `comparison_engine`, the code to do this is `comparison_engine.add_comparison(\"first_name\", \"jw\")`. If I wanted to also use a commonality weight, I would use `comparison_engine.add_comparison(\"first_name\", \"jw\", {\"comm_weight\": 'd', \"comm_col\": \"first_comm\"})`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see that I generate the comparison pipeline by creating lists of columns, comparisons, and extra arguments. This is the most efficient way of coding up the comparison pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the feature engineer\n",
    "fe = FeatureEngineer()\n",
    "cols = [\"marstat\", \"race\", \"rel\", \"mbp\", \"fbp\", \"first_sdxn\", \"last_sdxn\", \"bp\", \"county\",\n",
    "        \"immigration\", \"birth_year\", [\"res_lat\", \"res_lon\"], [\"bp_lat\", \"bp_lon\"],\n",
    "        [f\"first_vec{i}\" for i in range(2, 202)], [f\"last_vec{i}\" for i in range(2,202)],\n",
    "        \"first\", \"last\",\n",
    "        \"first\", \"last\"\n",
    "       ]\n",
    "\n",
    "#similarity functions\n",
    "col_comps = [\"exact match\"] * 9\n",
    "col_comps.extend([\"abs dist\"] * 2)\n",
    "col_comps.extend([\"euclidean dist\"] * 4)\n",
    "#col_comps.extend([\"geo dist\"] * 2)\n",
    "col_comps.extend([\"jw\"] * 2)\n",
    "col_comps.extend([\"trigram\"] * 2)\n",
    "#col_comps = [\"exact match\", \"exact match\" ..9x.., \"abs dist\", \"abs dist\", \"euclidean dist\", ]\n",
    "#extra arguments\n",
    "col_args = list({} for i in range(5))\n",
    "col_args.extend([{\"comm_weight\": \"d\", \"comm_col\": \"first_comm\"}, {\"comm_weight\": \"d\", \"comm_col\": \"last_comm\"},\n",
    "                 {\"comm_weight\": \"d\", \"comm_col\": \"bp_comm\"}])\n",
    "col_args.extend(list({} for i in range(7)))\n",
    "col_args.extend([{\"comm_weight\": \"d\", \"comm_col\": \"first_comm\"}, {\"comm_weight\": \"d\", \"comm_col\": \"last_comm\"}] * 2)\n",
    "assert len(cols) == len(col_comps) == len(col_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the above code actually does since it's a bit confusing:\n",
    "\n",
    "`col_comps = [\"exact match\", \"exact match\", \"exact match\", \"exact match\", \"exact match\", \"exact match\", \n",
    "              \"exact match\", \"exact match\", \"exact match\", \"abs dist\", \"abs dist\", \"euclidean dist\", \"euclidean dist\",\n",
    "              \"euclidean dist\", \"euclidean dist\", \"jw\", \"jw\", \"trigram\", \"trigram\"]`\n",
    "\n",
    "`col_args = [{}, {}, {}, {}, {}, {\"comm_weight\": \"d\", \"comm_col\": \"first_comm\"}, \n",
    "             {\"comm_weight\": \"d\", \"comm_col\": \"last_comm\"}, {\"comm_weight\": \"d\", \"comm_col\": \"bp_comm\"}, \n",
    "             {}, {}, {}, {}, {}, {}, {},  {\"comm_weight\": \"d\", \"comm_col\": \"first_comm\"}, \n",
    "             {\"comm_weight\": \"d\", \"comm_col\": \"last_comm\"}, {\"comm_weight\": \"d\", \"comm_col\": \"first_comm\"}, \n",
    "             {\"comm_weight\": \"d\", \"comm_col\": \"last_comm\"}]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, a comparison is the built using the $i$th element of every list. For example, the first comparison added is `fe.add_comparison(\"marstat\", \"exact match\", {})`. To create this, zip up all the lists and loop over them, as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j,k in zip(cols, col_comps, col_args):\n",
    "    fe.add_comparison(i, j, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't have to build the comparison engine like I did, but it does make the code more compact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a look at all of the available similarity functions I've coded up so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jw',\n",
       " 'abs dist',\n",
       " 'euclidean dist',\n",
       " 'geo dist',\n",
       " 'bigram',\n",
       " 'trigram',\n",
       " 'ngram',\n",
       " 'exact match']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fe.compares_avail.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I load in a model. For now, this is an old model that will give you junk predictions for our current data since I didn't bother cleaning the delaware data. Hopefully this will be improved so that you can see the actual predictions that the model is making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct XGBoostMatch\n",
    "with open(\"R:/JoePriceResearch/record_linking/projects/deep_learning/ml-record-linking/model.xgboost\", \"rb\") as file:\n",
    "    model = pkl.load(file)\n",
    "model.get_booster().feature_names = [f\"f{i}\" for i in range(19)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final step is to create and run a linker object. A linker object takes all of the previous parts and uses the similarity scores generated to either predict whether a record pair is a match or return a probability of a record pair being a match. Once again, the code is agnostic to which linking algorithm you use as long as the linking algorithm knows how to work with each of the objects created above.\n",
    "\n",
    "For this notebook, I use [XGBoost](https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/), a powerful adaptation of random forests. The last step is to use `run()` and specify the name of the file that you want your predictions saved to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb = XGBoostMatch(delaware_1910, delaware_1920, compares, fe, model)\n",
    "xgb.run(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we can run 22 million predictions per hour on a single machine, not including blocking and reading in data. 2/3 of the time is spent on the ngram comparison, so dropping that will lead to 66 million predictions per hour. You can test with your own data how much dropping ngram similarity impacts your precision and recall, but it probably won't be that big of a factor since there are many other name comparison metrics that you can use. (If you really need it, you can try optimizing my code for ngrams, which just calls a canned python module. Then submit a pull request to my github repo.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's the pipeline! The only step not shown is the blocking algorithm. We used SQL to block extremely quickly, but you can do it however you want. Maybe in the future we'll have a dedicated class for blocking, but that is not a priority. Use SQL if you have a very large dataset, or use Stata/Pandas for smaller datasets (merges in Stata are better than Pandas)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
