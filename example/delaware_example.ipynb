{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Splycer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I demonstrate the process of running Splycer from start to finish. First, load in all of the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "sys.path.append(\"R:/JoePriceResearch/record_linking/projects/deep_learning/ml-record-linking/\")\n",
    "from record_set import RecordDataFrame\n",
    "from pairs_set import PairsCOO\n",
    "from feature_engineer import FeatureEngineer\n",
    "from xgboost_match import XGBoostMatch\n",
    "os.chdir(\"R:/JoePriceResearch/record_linking/projects/deep_learning/ml-record-linking/example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell creates RecordDataFrame objects. This is essentially a wrapper for a dataframe object. This wrapper standardizes the way that Splycer retrieves records, allowing you to use any data structure that you want if you create a wrapper for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the census data.\n",
    "delaware_1910 = RecordDataFrame(2, pd.read_csv(\"delaware_1910.csv\", index_col=\"index\"))\n",
    "delaware_1920 = RecordDataFrame(3, pd.read_csv(\"delaware_1920.csv\", index_col=\"index\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "delaware_1910.df[\"first\"] = delaware_1910.df[\"first\"].mask(delaware_1910.df[\"first\"].isnull(), \"\")\n",
    "delaware_1920.df[\"first\"] = delaware_1920.df[\"first\"].mask(delaware_1920.df[\"first\"].isnull(), \"\")\n",
    "delaware_1910.df[\"last\"] = delaware_1910.df[\"last\"].mask(delaware_1910.df[\"last\"].isnull(), \"\")\n",
    "delaware_1920.df[\"last\"] = delaware_1920.df[\"last\"].mask(delaware_1920.df[\"last\"].isnull(), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    203320\n",
       "Name: first, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delaware_1910.df[\"first\"].isnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the pairs set. This represents post-blocking comparisons that we want to make. Once again, there is a wrapper for any data structure that you want. Here, I am using a COO matrix, which is a compressed matrix representation of compares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the compares set\n",
    "compares = pd.read_csv(\"delaware_compares.csv\")\n",
    "compares = PairsCOO(2,3, compares.index_1910.values, compares.index_1920.values, np.ones(compares.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the comparison engineer is the most intensive part of the pipeline. It is constructed by incrementally adding comparisons that you want to perform. For each comparison, you must specify the column name(s) of the record you want to compare; the type of comparison; and any extra arguments that the comparison takes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: suppose I wanted to compare first name using jaro-winkler score. After creating a ComparisonEngineer object `comparison_engine`, the code to do this is `comparison_engine.add_comparison(\"first_name\", \"jw\")`. If I wanted to also use a commonality weight, I would use `comparison_engine.add_comparison(\"first_name\", \"jw\", {\"comm_weight\": 'd', \"comm_col\": \"first_comm\"})`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see that I generate the comparison pipeline by creating lists of columns, comparisons, and extra arguments. This is the most efficient way of coding up the comparison pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct the feature engineer\n",
    "fe = FeatureEngineer()\n",
    "#FIXME add parent geo-coordinates\n",
    "cols = [\"marstat\", \"race\", \"rel\", \"mbp\", \"fbp\", \"first_sdxn\", \"last_sdxn\", \"bp\", \"county\",\n",
    "        \"immigration\", \"birth_year\", [\"res_lat\", \"res_lon\"], [\"bp_lat\", \"bp_lon\"],\n",
    "        [f\"first_vec{i}\" for i in range(2, 202)], [f\"last_vec{i}\" for i in range(2,202)],\n",
    "        \"first\", \"last\",\n",
    "        \"first\", \"last\"\n",
    "       ]\n",
    "\n",
    "col_comps = [\"exact match\"] * 9\n",
    "col_comps.extend([\"abs dist\"] * 2)\n",
    "col_comps.extend([\"euclidean dist\"] * 4)\n",
    "#col_comps.extend([\"geo dist\"] * 2)\n",
    "col_comps.extend([\"jw\"] * 2)\n",
    "col_comps.extend([\"trigram\"] * 2)\n",
    "\n",
    "col_args = list({} for i in range(5))\n",
    "col_args.extend([{\"comm_weight\": \"d\", \"comm_col\": \"first_comm\"}, {\"comm_weight\": \"d\", \"comm_col\": \"last_comm\"},\n",
    "                 {\"comm_weight\": \"d\", \"comm_col\": \"bp_comm\"}])\n",
    "col_args.extend(list({} for i in range(7)))\n",
    "col_args.extend([{\"comm_weight\": \"d\", \"comm_col\": \"first_comm\"}, {\"comm_weight\": \"d\", \"comm_col\": \"last_comm\"}] * 2)\n",
    "assert len(cols) == len(col_comps) == len(col_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j,k in zip(cols, col_comps, col_args):\n",
    "    fe.add_comparison(i, j, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct XGBoostMatch\n",
    "with open(\"R:/JoePriceResearch/record_linking/projects/deep_learning/ml-record-linking/model.xgboost\", \"rb\") as file:\n",
    "    model = pkl.load(file)\n",
    "model.get_booster().feature_names = [f\"f{i}\" for i in range(19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R:\\JoePriceResearch\\Python\\AnacondaNew\\envs\\ml_rec_env\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in log1p\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "test = XGBoostMatch(delaware_1910, delaware_1920, compares, fe, model)\n",
    "test.run(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently can run 3,237 compares/sec. Choosing a different ngram algorithm will lead to the largest optimizing gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<comparisons.BooleanMatch at 0x2b4a83b3630>,\n",
       " <comparisons.BooleanMatch at 0x2b4a83b3a20>,\n",
       " <comparisons.BooleanMatch at 0x2b4a83b3a90>,\n",
       " <comparisons.BooleanMatch at 0x2b4a83b3ac8>,\n",
       " <comparisons.BooleanMatch at 0x2b48418ff60>,\n",
       " <comparisons.BooleanMatch at 0x2b48418fdd8>,\n",
       " <comparisons.BooleanMatch at 0x2b48418fe80>,\n",
       " <comparisons.BooleanMatch at 0x2b48418f320>,\n",
       " <comparisons.BooleanMatch at 0x2b48418f4e0>,\n",
       " <comparisons.AbsDistance at 0x2b48418f748>,\n",
       " <comparisons.AbsDistance at 0x2b48418f668>,\n",
       " <comparisons.EuclideanDistance at 0x2b48418f550>,\n",
       " <comparisons.EuclideanDistance at 0x2b48418fb70>,\n",
       " <comparisons.EuclideanDistance at 0x2b4a8281048>,\n",
       " <comparisons.EuclideanDistance at 0x2b4a8281160>,\n",
       " <comparisons.JW at 0x2b4a82812e8>,\n",
       " <comparisons.JW at 0x2b4a82812b0>,\n",
       " <comparisons.TriGram at 0x2b4a8281518>,\n",
       " <comparisons.TriGram at 0x2b4a82815f8>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140729982233904"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0x00007FFE4099AD30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
